<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Yicong Zheng</title>
    <link>https://zycyc.github.io/projects/</link>
    <description>Recent content in Projects on Yicong Zheng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://zycyc.github.io/projects/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Photography</title>
      <link>https://zycyc.github.io/projects/photography/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://zycyc.github.io/projects/photography/</guid>
      <description>I enjoy taking photos everywhere I go.
They form the trace of my life and more importantly, my personal visual diary.
My cameras include Nikon D5200, iPhone 7, iPhone SE (emeritus) and Mi 4 (emeritus).</description>
    </item>
    
    <item>
      <title>Musical Memory</title>
      <link>https://zycyc.github.io/projects/mm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://zycyc.github.io/projects/mm/</guid>
      <description>How does human learn music? The one simple answer will be repeatedly listening, but what&amp;rsquo;s the underlying neural mechanism that supports this learning process? Better memory performance is predicted by higher representational similarity between encoding and retrieval (Xue, 2010), is memory of music the same? Beyond this question, I also want to find out how cortex and hippocampus represent musical features like tempo, pitch, rhythm, and how these two representations interact to depict music learning curve.</description>
    </item>
    
    <item>
      <title>Serial Dependence</title>
      <link>https://zycyc.github.io/projects/sd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://zycyc.github.io/projects/sd/</guid>
      <description>We tend to perceive the world around us as stable in space and time, a study by Fischer and Whitney (2014) suggests that human visual perception is serially dependent, using both prior and present input to inform perception at the present moment. Specifically, perceived orientation in an orientation judgment task is likely biased towards previously seen stimuli.
While I was an exchange student in UC Berkeley during Spring 2019 semester, I did replication and relevant experiments both on-line and off-line with Prof.</description>
    </item>
    
    <item>
      <title>Temporal Sequence</title>
      <link>https://zycyc.github.io/projects/ts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://zycyc.github.io/projects/ts/</guid>
      <description>Using similar paradigm with this paper (Crivelli-Decker, Hsieh, Clarke &amp;amp; Ranganath, 2018), we are trying to use EEG to unravel the role of the mysterious theta/alpha oscillations in temporal sequence learning, and the difference between healthy people and schizophrenia patients.
Previous studies have found decreased oscillatory power in the theta (4-7Hz) band at frontal sites following decisions about objects in &amp;ldquo;consistent sequences&amp;rdquo;, compared to &amp;ldquo;random&amp;rdquo; sequences; and also increased parieto-occipital alpha (10-13Hz) power before decisions.</description>
    </item>
    
  </channel>
</rss>